{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from https://huggingface.co/spaces/ml6team/post-processing-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulius/personal/Pauliaus/Lectures/Amsterdam/year2/semester1/deep-learning-for-nlp/assignments/.venv/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/paulius/personal/Pauliaus/Lectures/Amsterdam/year2/semester1/deep-learning-for-nlp/assignments/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/paulius/personal/Pauliaus/Lectures/Amsterdam/year2/semester1/deep-learning-for-nlp/assignments/.venv/lib/python3.8/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-22 14:03:32,981 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "\n",
    "from flair.nn import Classifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "def get_transformer_pipeline():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "    return pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n",
    "\n",
    "sentence_embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "ner_model = get_transformer_pipeline()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "flair_tagger = Classifier.load('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_entities_per_sentence(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    sentences = list(doc.sents)\n",
    "\n",
    "    entities_all_sentences = []\n",
    "    for sentence in sentences:\n",
    "        entities_this_sentence = []\n",
    "\n",
    "        # SPACY ENTITIES\n",
    "        for entity in sentence.ents:\n",
    "            entities_this_sentence.append(str(entity))\n",
    "\n",
    "        # FLAIR ENTITIES (CURRENTLY NOT USED)\n",
    "        sentence_entities = Sentence(str(sentence))\n",
    "        flair_tagger.predict(sentence_entities)\n",
    "        for entity in sentence_entities.get_spans('ner'):\n",
    "            entities_this_sentence.append(entity.text)\n",
    "\n",
    "        # XLM ENTITIES\n",
    "        entities_xlm = [entity[\"word\"] for entity in ner_model(str(sentence))]\n",
    "        for entity in entities_xlm:\n",
    "            entities_this_sentence.append(str(entity))\n",
    "\n",
    "        entities_all_sentences.append(entities_this_sentence)\n",
    "\n",
    "    return entities_all_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_compare_entities(source, summary):\n",
    "    all_entities_per_sentence = get_all_entities_per_sentence(source)\n",
    "    entities_source = list(itertools.chain.from_iterable(all_entities_per_sentence))\n",
    "\n",
    "    # if first_time:\n",
    "    #     article_content = st.session_state.article_text\n",
    "    #     all_entities_per_sentence = get_all_entities_per_sentence(article_content)\n",
    "    #     entities_article = list(itertools.chain.from_iterable(all_entities_per_sentence))\n",
    "    #     st.session_state.entities_article = entities_article\n",
    "    # else:\n",
    "    #     entities_article = st.session_state.entities_article\n",
    "\n",
    "    # summary_content = st.session_state.summary_output\n",
    "    all_entities_per_sentence = get_all_entities_per_sentence(summary)\n",
    "    entities_summary = list(itertools.chain.from_iterable(all_entities_per_sentence))\n",
    "\n",
    "    matched_entities = []\n",
    "    unmatched_entities = []\n",
    "    for entity in entities_summary:\n",
    "        if any(entity.lower() in substring_entity.lower() for substring_entity in entities_source):\n",
    "            matched_entities.append(entity)\n",
    "        elif any(\n",
    "                np.inner(sentence_embedding_model.encode(entity, show_progress_bar=False),\n",
    "                         sentence_embedding_model.encode(art_entity, show_progress_bar=False)) > 0.9 for\n",
    "                art_entity in entities_source):\n",
    "            matched_entities.append(entity)\n",
    "        else:\n",
    "            unmatched_entities.append(entity)\n",
    "\n",
    "    matched_entities = list(dict.fromkeys(matched_entities))\n",
    "    unmatched_entities = list(dict.fromkeys(unmatched_entities))\n",
    "\n",
    "    matched_entities_to_remove = []\n",
    "    unmatched_entities_to_remove = []\n",
    "\n",
    "    for entity in matched_entities:\n",
    "        for substring_entity in matched_entities:\n",
    "            if entity != substring_entity and entity.lower() in substring_entity.lower():\n",
    "                matched_entities_to_remove.append(entity)\n",
    "\n",
    "    for entity in unmatched_entities:\n",
    "        for substring_entity in unmatched_entities:\n",
    "            if entity != substring_entity and entity.lower() in substring_entity.lower():\n",
    "                unmatched_entities_to_remove.append(entity)\n",
    "\n",
    "    matched_entities_to_remove = list(dict.fromkeys(matched_entities_to_remove))\n",
    "    unmatched_entities_to_remove = list(dict.fromkeys(unmatched_entities_to_remove))\n",
    "\n",
    "    for entity in matched_entities_to_remove:\n",
    "        matched_entities.remove(entity)\n",
    "    for entity in unmatched_entities_to_remove:\n",
    "        unmatched_entities.remove(entity)\n",
    "\n",
    "    return matched_entities, unmatched_entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"Lionel Andrés Messi (born 24 June 1987) is an Argentine professional footballer who plays as a forward and captains both Spanish club Barcelona and the Argentina national team. Often considered as the best player in the world and widely regarded as one of the greatest players of all time, Messi has won a record six Ballon d'Or awards, a record six European Golden Shoes, and in 2020 was named to the Ballon d'Or Dream Team.\"\n",
    "summary = \"Lionel Andrés Messi (born 24 Aug 1997) is an Spanish professional footballer who plays as a forward and captains both Spanish club Barcelona and the Spanish national team.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched, unmatched = get_and_compare_entities(article, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['24', 'Spanish', 'Barcelona', 'Lionel Andrés Messi']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1997']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dependency(article: bool):\n",
    "    if article:\n",
    "        text = st.session_state.article_text\n",
    "        all_entities = get_all_entities_per_sentence(text)\n",
    "    else:\n",
    "        text = st.session_state.summary_output\n",
    "        all_entities = get_all_entities_per_sentence(text)\n",
    "    doc = nlp(text)\n",
    "    tok_l = doc.to_json()['tokens']\n",
    "    test_list_dict_output = []\n",
    "\n",
    "    sentences = list(doc.sents)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        start_id = sentence.start\n",
    "        end_id = sentence.end\n",
    "        for t in tok_l:\n",
    "            if t[\"id\"] < start_id or t[\"id\"] > end_id:\n",
    "                continue\n",
    "            head = tok_l[t['head']]\n",
    "            if t['dep'] == 'amod' or t['dep'] == \"pobj\":\n",
    "                object_here = text[t['start']:t['end']]\n",
    "                object_target = text[head['start']:head['end']]\n",
    "                if t['dep'] == \"pobj\" and str.lower(object_target) != \"in\":\n",
    "                    continue\n",
    "                # ONE NEEDS TO BE ENTITY\n",
    "                if object_here in all_entities[i]:\n",
    "                    identifier = object_here + t['dep'] + object_target\n",
    "                    test_list_dict_output.append({\"dep\": t['dep'], \"cur_word_index\": (t['id'] - sentence.start),\n",
    "                                                  \"target_word_index\": (t['head'] - sentence.start),\n",
    "                                                  \"identifier\": identifier, \"sentence\": str(sentence)})\n",
    "                elif object_target in all_entities[i]:\n",
    "                    identifier = object_here + t['dep'] + object_target\n",
    "                    test_list_dict_output.append({\"dep\": t['dep'], \"cur_word_index\": (t['id'] - sentence.start),\n",
    "                                                  \"target_word_index\": (t['head'] - sentence.start),\n",
    "                                                  \"identifier\": identifier, \"sentence\": str(sentence)})\n",
    "                else:\n",
    "                    continue\n",
    "    return test_list_dict_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_deps = check_dependency(False)\n",
    "article_deps = check_dependency(True)\n",
    "total_unmatched_deps = []\n",
    "for summ_dep in summary_deps:\n",
    "    if not any(summ_dep['identifier'] in art_dep['identifier'] for art_dep in article_deps):\n",
    "        total_unmatched_deps.append(summ_dep)\n",
    "if total_unmatched_deps:\n",
    "    for current_drawing_list in total_unmatched_deps:\n",
    "        render_dependency_parsing(current_drawing_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
