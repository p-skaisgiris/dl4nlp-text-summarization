{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallucination Metrics\n",
    "\n",
    "> DO NOT RUN ALL MODELS AT ONCE! Do FactSumm, then HuggingFace, then BLEURT separately as they all load pretty big models, it can overflow your RAM quite easily\n",
    "\n",
    "As always, there are a few out there, all of them bearing their own pros and cons. A few desirable criteria for a good metric are thus:\n",
    "- Bounded\n",
    "- Reproducible\n",
    "- Interpretable\n",
    "- Correlate with human judgement\n",
    "\n",
    "> Note: In the report we should maybe briefly discuss whether the metrics we pick meet these criteria "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing FactSumm\n",
    "\n",
    "> The Open IE method (the part where you need Java) still didn't work for me out-of-the-box and I don't care to fix it as we have good alternatives I think.\n",
    "\n",
    "One of FactSumm's dependencies has changed its interface over the years and FactSumm is not really a maintained project (however, it has a few ready-to-go implementations of metrics which make our lives easier). There is an easy fix, though.\n",
    "\n",
    "If you simply install FactSumm with\n",
    "```bash\n",
    "pip install factsumm\n",
    "```\n",
    "and try to run the example with Lionel Messi, you will likely run into a problem `KeyError: 'entities'`. All you have to do (as per [this](https://github.com/Huffon/factsumm/issues/36)) to fix this is \n",
    "1. navigate to your virtual / conda environment and find the installed 'factsumm' package folder\n",
    "2. open up `factsumm/utils/level_entity.py`\n",
    "3. Change `line_result = sentence.to_dict(tag_type=\"ner\")` to `line_result = sentence.get_spans('ner')`\n",
    "4. Change \n",
    "\n",
    "```python\n",
    "for entity in line_result[\"entities\"]:\n",
    "    if entity[\"text\"] not in cache:\n",
    "        dedup.append({\n",
    "            \"word\": entity[\"text\"],\n",
    "            \"entity\": entity[\"labels\"][0].value,\n",
    "            \"start\": entity[\"start_pos\"],\n",
    "            \"end\": entity[\"end_pos\"],\n",
    "        })\n",
    "        cache[entity[\"text\"]] = None\n",
    "result.append(dedup)\n",
    "```\n",
    "\n",
    "with \n",
    "```python\n",
    "for entity in line_result: \n",
    "    if entity.text not in cache:\n",
    "        dedup.append({\n",
    "            \"word\": entity.text, \n",
    "            \"entity\": entity.tag, \n",
    "            \"start\": entity.start_position, \n",
    "            \"end\": entity.end_position, \n",
    "        }) \n",
    "        cache[entity.text] = None\n",
    "result.append(dedup)\n",
    "```\n",
    "\n",
    "5. Done. Re-activate your virtual/conda environment for the changes to take place and re-run your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factsumm import FactSumm\n",
    "factsumm = FactSumm()\n",
    "article = \"Lionel Andrés Messi (born 24 June 1987) is an Argentine professional footballer who plays as a forward and captains both Spanish club Barcelona and the Argentina national team. Often considered as the best player in the world and widely regarded as one of the greatest players of all time, Messi has won a record six Ballon d'Or awards, a record six European Golden Shoes, and in 2020 was named to the Ballon d'Or Dream Team.\"\n",
    "summary = \"Lionel Andrés Messi (born 24 Aug 1997) is an Spanish professional footballer who plays as a forward and captains both Spanish club Barcelona and the Spanish national team.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Hallucinations are words generated by the model that are not supported by the source input. Deep learning based generation is prone to hallucinate unintended text. These hallucinations degrade system performance and fail to meet user expectations in many real-world scenarios. By applying entity matching, we can improve this problem for the downstream task of summary generation.\n",
    "\n",
    "In theory all entities in the summary (such as dates, locations and so on), should also be present in the article. Thus we can extract all entities from the summary and compare them to the entities of the original article, spotting potential hallucinations. The more unmatched entities we find, the lower the factualness score of the summary.\n",
    "\n",
    "From https://huggingface.co/spaces/ml6team/post-processing-summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical NLP scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FactSumm Triple-based fact score\n",
    "\n",
    "The most intuitive way to evaluating factual consistency is to count the fact overlap between generated summary and the source document, as shown in Figure 3. Facts are usually represented by relation triples (subject, relation, object), where the subject has a relation to the object. To resolve this problem, Goodrich et al. [2019] change to use relation extraction tools with fixed schema. Considering still the two sentences in Example 1, whether extracting from the source document or the summary, the extracted triples are (Hawaii, is the birthplace of, Obama) in fixed schema extraction. This helps extracted triples easier to compare.\n",
    "\n",
    "From https://arxiv.org/abs/2104.14839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triple-based Module (closed-scheme). Fact Score\n",
    "factsumm.extract_facts(article, summary, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML6 HuggingFace NER + Dependency Parsing scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulius/personal/Pauliaus/Lectures/Amsterdam/year2/semester1/deep-learning-for-nlp/assignments/.venv/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/paulius/personal/Pauliaus/Lectures/Amsterdam/year2/semester1/deep-learning-for-nlp/assignments/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/paulius/personal/Pauliaus/Lectures/Amsterdam/year2/semester1/deep-learning-for-nlp/assignments/.venv/lib/python3.8/site-packages/transformers/pipelines/token_classification.py:169: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-23 15:46:34,218 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "\n",
    "from flair.nn import Classifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "def get_transformer_pipeline():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"xlm-roberta-large-finetuned-conll03-english\")\n",
    "    return pipeline(\"ner\", model=model, tokenizer=tokenizer, grouped_entities=True)\n",
    "\n",
    "sentence_embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "ner_model = get_transformer_pipeline()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "flair_tagger = Classifier.load('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_entities_per_sentence(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    sentences = list(doc.sents)\n",
    "\n",
    "    entities_all_sentences = []\n",
    "    for sentence in sentences:\n",
    "        entities_this_sentence = []\n",
    "\n",
    "        # SPACY ENTITIES\n",
    "        for entity in sentence.ents:\n",
    "            entities_this_sentence.append(str(entity))\n",
    "\n",
    "        # FLAIR ENTITIES (CURRENTLY NOT USED)\n",
    "        sentence_entities = Sentence(str(sentence))\n",
    "        flair_tagger.predict(sentence_entities)\n",
    "        for entity in sentence_entities.get_spans('ner'):\n",
    "            entities_this_sentence.append(entity.text)\n",
    "\n",
    "        # XLM ENTITIES\n",
    "        entities_xlm = [entity[\"word\"] for entity in ner_model(str(sentence))]\n",
    "        for entity in entities_xlm:\n",
    "            entities_this_sentence.append(str(entity))\n",
    "\n",
    "        entities_all_sentences.append(entities_this_sentence)\n",
    "\n",
    "    return entities_all_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"Lionel Andrés Messi (born 20 June 1987) is an Argentine professional footballer who plays as a forward and captains both Spanish club Barcelona and the Argentina national team. Often considered as the best player in the world and widely regarded as one of the greatest players of all time, Messi has won a record six Ballon d'Or awards, a record six European Golden Shoes, and in 2020 was named to the Ballon d'Or Dream Team.\"\n",
    "summary = \"Lionel Andrés Messi (born 24 Aug 1997) is an Spanish professional footballer who plays as a forward and captains both Spanish club Barcelona and the Spanish national team.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched, unmatched = get_and_compare_entities(article, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spanish', 'Barcelona', 'Lionel Andrés Messi']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['24', '1997']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dependency(text):\n",
    "    all_entities = get_all_entities_per_sentence(text)\n",
    "    doc = nlp(text)\n",
    "    tok_l = doc.to_json()['tokens']\n",
    "    test_list_dict_output = []\n",
    "\n",
    "    sentences = list(doc.sents)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        start_id = sentence.start\n",
    "        end_id = sentence.end\n",
    "        for t in tok_l:\n",
    "            if t[\"id\"] < start_id or t[\"id\"] > end_id:\n",
    "                continue\n",
    "            head = tok_l[t['head']]\n",
    "            if t['dep'] == 'amod' or t['dep'] == \"pobj\":\n",
    "                object_here = text[t['start']:t['end']]\n",
    "                object_target = text[head['start']:head['end']]\n",
    "                if t['dep'] == \"pobj\" and str.lower(object_target) != \"in\":\n",
    "                    continue\n",
    "                # ONE NEEDS TO BE ENTITY\n",
    "                if object_here in all_entities[i]:\n",
    "                    identifier = object_here + t['dep'] + object_target\n",
    "                    test_list_dict_output.append({\"dep\": t['dep'], \"cur_word_index\": (t['id'] - sentence.start),\n",
    "                                                  \"target_word_index\": (t['head'] - sentence.start),\n",
    "                                                  \"identifier\": identifier, \"sentence\": str(sentence)})\n",
    "                elif object_target in all_entities[i]:\n",
    "                    identifier = object_here + t['dep'] + object_target\n",
    "                    test_list_dict_output.append({\"dep\": t['dep'], \"cur_word_index\": (t['id'] - sentence.start),\n",
    "                                                  \"target_word_index\": (t['head'] - sentence.start),\n",
    "                                                  \"identifier\": identifier, \"sentence\": str(sentence)})\n",
    "                else:\n",
    "                    continue\n",
    "    return test_list_dict_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in summary but not in source:\n",
      "\n",
      "['Spanish', 'footballer']\n",
      "['Spanish', 'team']\n"
     ]
    }
   ],
   "source": [
    "source_deps = check_dependency(article)\n",
    "summary_deps = check_dependency(summary)\n",
    "total_unmatched_deps = []\n",
    "for summ_dep in summary_deps:\n",
    "    if not any(summ_dep['identifier'] in art_dep['identifier'] for art_dep in source_deps):\n",
    "        total_unmatched_deps.append(summ_dep)\n",
    "\n",
    "print(\"Found in summary but not in source:\\n\")\n",
    "for unmatched_dep in total_unmatched_deps:\n",
    "    print(unmatched_dep[\"identifier\"].split(unmatched_dep[\"dep\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we have found that **there are specific dependencies that are often an indication of a wrongly constructed sentence** when there is no article match. We (currently) use 2 common dependencies which - when present in the summary but not in the article - are highly indicative of factualness errors. Furthermore, we only check dependencies between an existing **entity** and its direct connections. Below we highlight all unmatched dependencies that satisfy the discussed constraints. We also discuss the specific results for the currently selected example article.\n",
    "\n",
    "Taken from https://huggingface.co/spaces/ml6team/post-processing-summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question-Answer Question-Generation Score\n",
    "\n",
    "Inspired by other question answering (QA) based automatic metrics in text summarization, Wang et al.; Durmus et al. [2020; 2020] propose QA based factual consistency evaluation metrics QAGS and FEQA separately. These two metrics are all based on the intuition that if we ask questions about a summary and its source document, we will receive similar answers if the summary is factually consistent with the source document. As illustrated in Figure 4, they are all consist of three steps: (1) Given a generated summary, a question generation (QG) model generates a set of questions about the summary, standard answers of which are named entities and key phrases in the summary. (2) Then using question answering (QA) model to answers these questions given the source document. (3) A factual consistency score is computed based on the similarity of corresponding answers. Because evaluating factual consistency at entity-level, these methods are more interpretable than textual-entailment-based methods. The reading comprehension ability of QG and QA models brings these methods promising performance in this task. However, these approaches are computationally expensive.\n",
    "\n",
    "From https://arxiv.org/abs/2104.14839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'factsumm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# QA-based Module. QAGS Score\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m factsumm\u001b[39m.\u001b[39mextract_qas(article, summary, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'factsumm' is not defined"
     ]
    }
   ],
   "source": [
    " # QA-based Module. QAGS Score\n",
    " factsumm.extract_qas(article, summary, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE score\n",
    "\n",
    "*Relevant for ROUGE*\n",
    "\n",
    "Besides the above methods specially designed, there are also several simple but effective methods to evaluate factual consistency, which are usually used as baselines. Durmus et al. [2020] propose that a straightforward metric for factual consistency is the word overlap or semantic similarity between the summary sentence and the source document. The word overlap-based metrics compute ROUGE [Lin, 2004], BLEU [Papineni et al., 2002], between the output summary sentence and each source sentence. And then taking the average score or maximum score across all the source sentences.\n",
    "\n",
    "*Relevant for BERT score / BLEURT*\n",
    "\n",
    "The semantic similarity-based metric is similar to word overlap-based methods. Instead of using ROUGE or BLEU, this method uses BERTScore [Zhang* et al., 2020a]. These two types of methods show a baseline level of effectiveness. And experiments in Durmus et al. [2020] show that word overlap-based methods work better in lowly abstractive summarization datasets like CNN/DM [Hermann et al., 2015], semantic similarity-based method works better in highly abstractive summarization datasets like XSum [Narayan et al., 2018]. Abstractiveness of the summarization dataset means the extent how abstract the reference summaries are against the source documents. Extremely, the summarization dataset is the least abstractive if all the reference summaries of which are directly extracted from the source document.\n",
    "\n",
    "From https://arxiv.org/abs/2104.14839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE-based Module. Avg. ROUGE-1, Avg. ROUGE-2, Avg. ROUGE-L\n",
    "factsumm.calculate_rouge(article, summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT score\n",
    "\n",
    "Through meta-evaluation, Koto et al. [2020] find that the semantic similarity-based method could reach stateof-the-art performance for factual consistency evaluation by searching optimal model parameters (i.e. model layers of pre-trained language model in BERTScore) in highly abstractive summarization dataset XSum [Narayan et al., 2018]. Even so, the correlation with human evaluation is not more than 0.5. Therefore, factual consistency evaluation is still an open issue in exploration.\n",
    "\n",
    "From https://arxiv.org/abs/2104.14839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTScore Module. BERTScore Score\n",
    "factsumm.calculate_bert_score(article, summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEURT\n",
    "\n",
    "BLEURT is an evaluation metric for Natural Language Generation. It takes a pair of sentences as input, a reference and a candidate, and it returns a score that indicates to what extent the candidate is fluent and conveys the meaning of the reference. It is comparable to sentence-BLEU, BERTscore, and COMET.\n",
    "\n",
    "BLEURT is a trained metric, that is, it is a regression model trained on ratings data. The model is based on BERT and RemBERT.\n",
    "\n",
    "An overview of BLEURT can be found in our our [blog post](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html). Further details are provided in the ACL paper [BLEURT: Learning Robust Metrics for Text Generation](https://arxiv.org/abs/2004.04696) and our EMNLP paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "I followed the installation instructions from [here](https://github.com/google-research/bleurt/tree/master). \n",
    "\n",
    "```bash\n",
    "git clone https://github.com/google-research/bleurt.git\n",
    "cd bleurt\n",
    "pip install .\n",
    "```\n",
    "\n",
    "and then, in the `bleurt` folder:\n",
    "\n",
    "```bash\n",
    "# Smaller version of BLEURT\n",
    "wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20-D3.zip .\n",
    "unzip BLEURT-20-D3.zip\n",
    "\n",
    "# If your PC can handle it, do \n",
    "wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip .\n",
    "unzip BLEURT-20.zip\n",
    "```\n",
    "\n",
    "to read more about the available, check this: https://github.com/google-research/bleurt/blob/master/checkpoints.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from bleurt import score\n",
    "\n",
    "checkpoint = \"../bleurt/BLEURT-20-D3\"\n",
    "references = [\"Bud Powell was a legendary pianist.\", \"Bud Powell was a legendary pianist.\", \"Bud Powell was a legendary pianist.\"]\n",
    "candidates = [\"Bud Powell was a legendary pianist.\", \"Bud Powell was a good keys player\", \"Bud Powell was a New Yorker\"]\n",
    "\n",
    "scorer = score.BleurtScorer(checkpoint)\n",
    "scores = scorer.score(references=references, candidates=candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9509367346763611, 0.4507386088371277, 0.3503122925758362]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SUMMAC score\n",
    "\n",
    "SUMMAC (Summary Consistency; Laban et al., 2021) is focused on evaluating factual consistency in summarization. They use NLI for detecting in- consistencies by splitting the document and summary into sentences and computing the entailment probabilities on all document/summary sentence pairs, where the premise is a document sentence and the hypothesis is a summary sentence. They aggregate the NLI scores for all pairs by either tak- ing the maximum score per summary sentence and averaging (SCZS) or by training a convolutional neural network to aggregate the scores (SCConv). We use the publicly available implementation.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "{'scores': [0.20296768844127655]}\n"
     ]
    }
   ],
   "source": [
    "from summac.model_summac import SummaCZS, SummaCConv\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "article = \"Lionel Andrés Messi (born 24 June 1987) is an Argentine professional footballer who plays as a forward and captains both Spanish club Barcelona and the Argentina national team. Often considered as the best player in the world and widely regarded as one of the greatest players of all time, Messi has won a record six Ballon d'Or awards, a record six European Golden Shoes, and in 2020 was named to the Ballon d'Or Dream Team.\"\n",
    "summary = \"Lionel Andrés Messi (born 24 Aug 1997) is an Spanish professional footballer who plays as a forward and captains both Spanish club Barcelona and the Spanish national team.\"\n",
    "\n",
    "model_zs = SummaCZS(granularity=\"sentence\", model_name=\"vitc\", device=\"cpu\") # If you have a GPU: switch to: device=\"cuda\"\n",
    "model_conv = SummaCConv(models=[\"vitc\"], bins='percentile', granularity=\"sentence\", nli_labels=\"e\", device=\"cpu\", start_file=\"default\", agg=\"mean\")\n",
    "\n",
    "score_zs = model_zs.score([article], [summary])\n",
    "score_conv = model_conv.score([article], [summary])\n",
    "\n",
    "print(score_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUMMAC will produce a metric score that reflects the quality of the generated hypotheses. You can set a threshold score below which you consider a hypothesis to be indicative of hallucination. For instance, if the SUMMAC score falls below a certain threshold, it may suggest that the hypothesis contains hallucinatory information.\n",
    "\n",
    "It's important to note that the success of using SUMMAC or any other metric for hallucination detection depends on the quality of the ground truth labels and the design of your evaluation dataset. Additionally, SUMMAC may not explicitly identify hallucination, but it can assess the overall quality of generated text in terms of semantic coherence and alignment with the input premise. The presence of hallucinatory information may result in lower SUMMAC scores, but a comprehensive assessment might require additional metrics or human evaluation for a complete understanding of hallucination in machine-generated text.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
