{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallucination Metrics\n",
    "\n",
    "As always, there are a few out there, all of them bearing their own pros and cons. A few desirable criteria for a good metric are thus:\n",
    "- Bounded\n",
    "- Reproducible\n",
    "- Interpretable\n",
    "- Correlate with human judgement\n",
    "\n",
    "> Note: In the report we should maybe briefly discuss whether the metrics we pick meet these criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing FactSumm\n",
    "\n",
    "One of FactSumm's dependencies has changed its interface over the years and FactSumm is not really a maintained project (however, it has a few ready-to-go implementations of metrics which make our lives easier). There is an easy fix, though.\n",
    "\n",
    "If you simply install FactSumm with\n",
    "```bash\n",
    "pip install factsumm\n",
    "```\n",
    "and try to run the example with Lionel Messi, you will likely run into a problem `KeyError: 'entities'`. All you have to do (as per [this](https://github.com/Huffon/factsumm/issues/36)) to fix this is \n",
    "1. navigate to your virtual / conda environment and find the installed 'factsumm' package folder\n",
    "2. open up `factsumm/utils/level_entity.py`\n",
    "3. Change `line_result = sentence.to_dict(tag_type=\"ner\")` to `line_result = sentence.get_spans('ner')`\n",
    "4. Change \n",
    "\n",
    "```python\n",
    "for entity in line_result[\"entities\"]:\n",
    "    if entity[\"text\"] not in cache:\n",
    "        dedup.append({\n",
    "            \"word\": entity[\"text\"],\n",
    "            \"entity\": entity[\"labels\"][0].value,\n",
    "            \"start\": entity[\"start_pos\"],\n",
    "            \"end\": entity[\"end_pos\"],\n",
    "        })\n",
    "        cache[entity[\"text\"]] = None\n",
    "result.append(dedup)\n",
    "```\n",
    "\n",
    "with \n",
    "```python\n",
    "for entity in line_result: \n",
    "    if entity.text not in cache:\n",
    "        dedup.append({\n",
    "            \"word\": entity.text, \n",
    "            \"entity\": entity.tag, \n",
    "            \"start\": entity.start_position, \n",
    "            \"end\": entity.end_position, \n",
    "        }) \n",
    "        cache[entity.text] = None\n",
    "result.append(dedup)\n",
    "```\n",
    "\n",
    "5. Done. Re-activate your virtual/conda environment for the changes to take place and re-run your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not-working metrics\n",
    "\n",
    "Open IE method (the part where you need Java) still didn't work for me out-of-the-box and I don't care to fix it as we have good alternatives I think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factsumm import FactSumm\n",
    "factsumm = FactSumm()\n",
    "article = \"Lionel Andrés Messi (born 24 June 1987) is an Argentine professional footballer who plays as a forward and captains both Spanish club Barcelona and the Argentina national team. Often considered as the best player in the world and widely regarded as one of the greatest players of all time, Messi has won a record six Ballon d'Or awards, a record six European Golden Shoes, and in 2020 was named to the Ballon d'Or Dream Team.\"\n",
    "summary = \"Lionel Andrés Messi (born 24 Aug 1997) is an Spanish professional footballer who plays as a forward and captains both Spanish club Barcelona and the Spanish national team.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triple-based fact score\n",
    "\n",
    "The most intuitive way to evaluating factual consistency is to count the fact overlap between generated summary and the source document, as shown in Figure 3. Facts are usually represented by relation triples (subject, relation, object), where the subject has a relation to the object. To resolve this problem, Goodrich et al. [2019] change to use relation extraction tools with fixed schema. Considering still the two sentences in Example 1, whether extracting from the source document or the summary, the extracted triples are (Hawaii, is the birthplace of, Obama) in fixed schema extraction. This helps extracted triples easier to compare.\n",
    "\n",
    "From https://arxiv.org/abs/2104.14839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triple-based Module (closed-scheme). Fact Score\n",
    "factsumm.extract_facts(article, summary, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question-Answer Question-Generation Score\n",
    "\n",
    "Inspired by other question answering (QA) based automatic metrics in text summarization, Wang et al.; Durmus et al. [2020; 2020] propose QA based factual consistency evaluation metrics QAGS and FEQA separately. These two metrics are all based on the intuition that if we ask questions about a summary and its source document, we will receive similar answers if the summary is factually consistent with the source document. As illustrated in Figure 4, they are all consist of three steps: (1) Given a generated summary, a question generation (QG) model generates a set of questions about the summary, standard answers of which are named entities and key phrases in the summary. (2) Then using question answering (QA) model to answers these questions given the source document. (3) A factual consistency score is computed based on the similarity of corresponding answers. Because evaluating factual consistency at entity-level, these methods are more interpretable than textual-entailment-based methods. The reading comprehension ability of QG and QA models brings these methods promising performance in this task. However, these approaches are computationally expensive.\n",
    "\n",
    "From https://arxiv.org/abs/2104.14839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'factsumm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# QA-based Module. QAGS Score\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m factsumm\u001b[39m.\u001b[39mextract_qas(article, summary, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'factsumm' is not defined"
     ]
    }
   ],
   "source": [
    " # QA-based Module. QAGS Score\n",
    " factsumm.extract_qas(article, summary, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE score\n",
    "\n",
    "Besides the above methods specially designed, there are also several simple but effective methods to evaluate factual consistency, which are usually used as baselines. Durmus et al. [2020] propose that a straightforward metric for factual consistency is the word overlap or semantic similarity between the summary sentence and the source document. The word overlap-based metrics compute ROUGE [Lin, 2004], BLEU [Papineni et al., 2002], between the output summary sentence and each source sentence. And then taking the average score or maximum score across all the source sentences.\n",
    "\n",
    "From https://arxiv.org/abs/2104.14839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE-based Module. Avg. ROUGE-1, Avg. ROUGE-2, Avg. ROUGE-L\n",
    "factsumm.calculate_rouge(article, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTScore Module. BERTScore Score\n",
    "factsumm.calculate_bert_score(article, summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
